{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Metrics extraction and model selecction\n",
    "Import detectron2 and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "9_FzH13EjseR",
    "outputId": "144b1173-978e-485c-f83a-5d2d53a2bf3b"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.evaluation import PascalVOCDetectionEvaluator\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.visualizer import ColorMode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2bjrfb2LDeo"
   },
   "source": [
    "\n",
    "# Paths and checks for the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RGB=False\n",
    "%env first_iter=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DIR=/eos/jeodpp/data/projects/REFOCUS/data/swalim_v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the paths to run the evaluation stage and load the test or validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6C4n8QaoFRX",
    "outputId": "477a8258-bfc5-44cb-88bb-c841baca7a76"
   },
   "outputs": [],
   "source": [
    "#Add the path to check\n",
    "   \n",
    "if  os.getenv('first_iter') == 'True':\n",
    "    if os.getenv('RGB') == 'False':\n",
    "        path_ann = '{}/inputs/cvat/pancro_first_iter/'.format(os.getenv('DIR'))\n",
    "        path_imgs = '{}/inputs/pancro_first'.format(os.getenv('DIR'))\n",
    "        results_path = '{}/outputs/first_iter/pancro/'.format(os.getenv('DIR'))\n",
    "    else:\n",
    "        path_ann = '{}/inputs/cvat/rgb_first_iter/'.format(os.getenv('DIR'))\n",
    "        path_imgs = '{}/inputs/rgb_first'.format(os.getenv('DIR'))\n",
    "        results_path = '{}/outputs/first_iter/rgb/'.format(os.getenv('DIR'))\n",
    "\n",
    "    #For the model selection we use the validation set, for final metrics test\n",
    "    test = path_ann+'val.json'\n",
    "    with open(path_ann+'val.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    aux = data['images']\n",
    "    aux2 = data['annotations']\n",
    "    print(\"Number of images for validation and model selection: {}, number of annotations:{}\".format(len(aux), len(aux2)))\n",
    "\n",
    "else:\n",
    "    if os.getenv('RGB') == 'False':\n",
    "        path_ann = '{}/inputs/cvat/pancro_astrid_300/'.format(os.getenv('DIR'))\n",
    "        path_imgs = '{}/pancro_300'.format(os.getenv('DIR'))\n",
    "        results_path = '{}/outputs/second_iter/pancro_300/'.format(os.getenv('DIR'))\n",
    "    else:\n",
    "        path_ann = '{}/inputs/cvat/rgb_astrid_300/'.format(os.getenv('DIR'))\n",
    "        path_imgs = '{}/RGB_300'.format(os.getenv('DIR'))\n",
    "        results_path = '{}/outputs/second_iter/rgb/'.format(os.getenv('DIR'))\n",
    "\n",
    "    #For the model selection we use the validation set, for final metrics test\n",
    "    test = path_ann+'val.json'\n",
    "    with open(path_ann+'val.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    aux = data['images']\n",
    "    aux2 = data['annotations']\n",
    "    print(\"Number of images for test and final metrics: {}, number of annotations:{}\".format(len(aux), len(aux2)))\n",
    "\n",
    "\n",
    "\n",
    "train = path_ann+'train.json'\n",
    "with open(path_ann+'train.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "aux = data['images']\n",
    "aux2 = data['annotations']\n",
    "print(\"Number of images for train: {}, number of annotations:{}\".format(len(aux), len(aux2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVJoOm6LVJwW"
   },
   "source": [
    "# Evaluation\n",
    "Register the dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Lnkg1PByUjGQ",
    "outputId": "dac35700-9a68-435b-bce7-034b56fb4fbe"
   },
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "try:\n",
    "    DatasetCatalog.remove(\"swalim_train\")\n",
    "    DatasetCatalog.remove(\"swalim_val\")\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWknKqWTWIw9",
    "outputId": "2235929a-e5ea-41cd-d098-b5a878144f60"
   },
   "outputs": [],
   "source": [
    "register_coco_instances(\"swalim_train\", {}, train, path_imgs)\n",
    "register_coco_instances(\"swalim_val\", {}, test, path_imgs)\n",
    "\n",
    "MetadataCatalog.get(\"swalim_val\")\n",
    "DatasetCatalog.get(\"swalim_val\")\n",
    "swalim_metadata = MetadataCatalog.get(\"swalim_train\")\n",
    "dataset_dicts = DatasetCatalog.get(\"swalim_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "Load the fuctions needed for the inference\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPpMOq2xPbwg"
   },
   "outputs": [],
   "source": [
    "def load_conf_file(path):\n",
    "    config_file_path = path\n",
    "    weights_path = \"{}model_final.pth\".format(str(path).split('config.yaml')[0])\n",
    "    print(weights_path)\n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(config_file_path)\n",
    "\n",
    "    cfg.DATASETS.TRAIN = (\"swalimRGB_train\", )\n",
    "    cfg.DATASETS.TEST = (\"swalim_val\", )\n",
    "    cfg.DATALOADER.NUM_WORKERS = 4\n",
    "    cfg.MODEL.WEIGHTS = weights_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "    return cfg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcjS-ZCgYEtc"
   },
   "outputs": [],
   "source": [
    "def Evaluator(cfg, predictor, dataset_dicts, IoUThresh, dataset):\n",
    "    evaluator = COCOEvaluator(dataset)\n",
    "    val_loader = build_detection_test_loader(cfg, dataset)\n",
    "    AP_res = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "\n",
    "    #Calc of precision, recall , f1score at an IoU of IoUThresh\n",
    "    count = 0\n",
    "    img_res = {}\n",
    "    for d in dataset_dicts:\n",
    "        count += 1\n",
    "        ann = d[\"annotations\"]\n",
    "        inst = detectron2.data.detection_utils.annotations_to_instances(ann, (d['width'], d['height']),\n",
    "                                                                      mask_format='polygon')\n",
    "        bboxes_gt = inst.gt_boxes.tensor.cpu().numpy()\n",
    "        im = cv2.imread(d['file_name'])\n",
    "        outputs = predictor(im)\n",
    "        bboxes_dt = outputs[\"instances\"].pred_boxes.tensor.cpu().numpy()\n",
    "        img_res[d[\"image_id\"]] = get_single_image_results(bboxes_gt, bboxes_dt, IoUThresh)\n",
    "    pr_rc = calc_precision_recall(img_res)\n",
    "    return [list(AP_res.items())[0][1],pr_rc['true_positive'], pr_rc['false_positive'], pr_rc['false_negative'], pr_rc['recall'], pr_rc['precision'], pr_rc['f1']]\n",
    "\n",
    "\n",
    "def calc_iou(gt_bbox, pred_bbox):\n",
    "    '''\n",
    "    This function takes the predicted bounding box and ground truth bounding box and\n",
    "    return the IoU ratio\n",
    "    '''\n",
    "    x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt = gt_bbox\n",
    "    x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p = pred_bbox\n",
    "\n",
    "    if (x_topleft_gt > x_bottomright_gt) or (y_topleft_gt > y_bottomright_gt):\n",
    "        raise AssertionError(\"Ground Truth Bounding Box is not correct\")\n",
    "    if (x_topleft_p > x_bottomright_p) or (y_topleft_p > y_bottomright_p):\n",
    "        raise AssertionError(\"Predicted Bounding Box is not correct\", x_topleft_p, x_bottomright_p, y_topleft_p,\n",
    "                             y_bottomright_gt)\n",
    "\n",
    "    # if the GT bbox and predcited BBox do not overlap then iou=0\n",
    "    if (x_bottomright_gt < x_topleft_p):\n",
    "        # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox\n",
    "        return 0.0\n",
    "    if (y_bottomright_gt < y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox\n",
    "        return 0.0\n",
    "    if (x_topleft_gt > x_bottomright_p):  # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox\n",
    "        return 0.0\n",
    "    if (y_topleft_gt > y_bottomright_p):  # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox\n",
    "        return 0.0\n",
    "\n",
    "    GT_bbox_area = (x_bottomright_gt - x_topleft_gt + 1) * (y_bottomright_gt - y_topleft_gt + 1)\n",
    "    Pred_bbox_area = (x_bottomright_p - x_topleft_p + 1) * (y_bottomright_p - y_topleft_p + 1)\n",
    "\n",
    "    x_top_left = np.max([x_topleft_gt, x_topleft_p])\n",
    "    y_top_left = np.max([y_topleft_gt, y_topleft_p])\n",
    "    x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])\n",
    "    y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])\n",
    "\n",
    "    intersection_area = (x_bottom_right - x_top_left + 1) * (y_bottom_right - y_top_left + 1)\n",
    "\n",
    "    union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)\n",
    "\n",
    "    return intersection_area / union_area\n",
    "\n",
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices = range(len(pred_boxes))\n",
    "    all_gt_indices = range(len(gt_boxes))\n",
    "    if len(all_pred_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "    if len(all_gt_indices) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "\n",
    "    gt_idx_thr = []\n",
    "    pred_idx_thr = []\n",
    "    ious = []\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou = calc_iou(gt_box, pred_box)\n",
    "\n",
    "            if iou > iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "    iou_sort = np.argsort(ious)[::1]\n",
    "    if len(iou_sort) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "    else:\n",
    "        gt_match_idx = []\n",
    "        pred_match_idx = []\n",
    "        for idx in iou_sort:\n",
    "            gt_idx = gt_idx_thr[idx]\n",
    "            pr_idx = pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if (gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp = len(gt_match_idx)\n",
    "        fp = len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "    return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}\n",
    "\n",
    "\n",
    "def calc_precision_recall(image_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "    Returns:\n",
    "        dictionary:\n",
    "        {'true_positive':true_positive, 'false_positive':false_positive, 'false_negative':false_negative, 'precision':precision, 'recall':recall,  'f1':f1_score}\n",
    "    \"\"\"\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    for img_id, res in image_results.items():\n",
    "        true_positive += res['true_positive']\n",
    "        false_positive += res['false_positive']\n",
    "        false_negative += res['false_negative']\n",
    "        try:\n",
    "            precision = true_positive / (true_positive + false_positive)\n",
    "        except ZeroDivisionError:\n",
    "            precision = 0.0\n",
    "        try:\n",
    "            recall = true_positive / (true_positive + false_negative)\n",
    "        except ZeroDivisionError:\n",
    "            recall = 0.0\n",
    "        try:\n",
    "            f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "        except ZeroDivisionError:\n",
    "            f1_score = 0.0\n",
    "\n",
    "    res = {'true_positive': true_positive, 'false_positive': false_positive, 'false_negative': false_negative,\n",
    "           'precision': precision, 'recall': recall, 'f1': f1_score}\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e4vdDIOXyxF"
   },
   "source": [
    "Set up the predictor to work on the test set.\n",
    "\n",
    "Loop though the output to get the list of models done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uHz-eRAz0GO",
    "outputId": "276c7879-a37a-4c26-9a1f-1b215164f065",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We set the IoU at 0.5\n",
    "IoUThresh = 0.5\n",
    "\n",
    "\n",
    "models = []\n",
    "dataset_dicts = DatasetCatalog.get(\"swalim_val\")\n",
    "full_results = pd.DataFrame(columns=['model', 'LR', 'momentum', \\\n",
    "                                     'RBF', 'AP', 'AP50', 'AP75', \\\n",
    "                                     'TP', 'FP', 'FN', 'Precision', \\\n",
    "                                     'Recall', 'F_score', 'Path'])\n",
    "\n",
    "for path in Path(results_path).rglob('*.yaml'):\n",
    "    models.append(path)\n",
    "    aux = \"{}model_final.pth\".format(str(path).split('config.yaml')[0])\n",
    "    cfg = load_conf_file(str(path))\n",
    "\n",
    "    if os.path.exists(aux):\n",
    "        predictor = DefaultPredictor(cfg)\n",
    "        res = Evaluator(cfg, predictor, dataset_dicts, IoUThresh, \"swalim_val\")\n",
    "        mod = 'Faster_rcnn101X'\n",
    "        full_results = full_results.append({'model': mod, 'LR': cfg.SOLVER.BASE_LR, 'momentum': cfg.SOLVER.MOMENTUM,'RBF': cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE\\\n",
    "                                          , 'AP': res[0]['AP'], 'AP50': res[0]['AP50'], 'AP75': res[0]['AP75'], 'TP': res[1] \\\n",
    "                                          , 'FP': res[2], 'FN': res[3], 'Precision': res[5]\\\n",
    "                                          , 'Recall': res[4], 'F_score': res[6], 'Path': path}, ignore_index=True)\n",
    "    \n",
    "    else:\n",
    "        print('Path to the model: {} not found'.format(aux))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results of the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "M-aBsKZaE88U",
    "outputId": "4d23c538-74c4-4602-b9f1-21e4199a696c"
   },
   "outputs": [],
   "source": [
    "print(full_results.dropna().sort_values(by=['F_score'], ascending=False))\n",
    "\n",
    "full_results = full_results.loc[:, ~full_results.columns.str.contains('^Unnamed')]\n",
    "\n",
    "full_results.to_csv('{}final_results.csv'.format(results_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB3sVppdgJAs"
   },
   "source": [
    "Plot the accuracies on the hypertune space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "oMjHodyZgH1q",
    "outputId": "c9adbe5f-0d50-4963-fec8-fd1d55a3d71e"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly\n",
    "\n",
    "df = full_results.dropna()\n",
    "fig = px.parallel_coordinates(df, color=\"F_score\", labels={\"F_score\": \"F1-Score\",\n",
    "                \"model\": \"Model\", \"LR\": \"Learning rate\",\n",
    "                \"momentum\": \"Momentum\", \"RBF\": \"Batch Size\", \"AP50\": \"Average precision at .5\",\"AP75\": \"Average precision at .75\",})\n",
    "fig.show()\n",
    "fig.write_html(\"{}/hypertune-plot.html\".format(results_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EbaGD0iVxcq"
   },
   "source": [
    "TODO anadir plots de https://towardsdatascience.com/evaluating-performance-of-an-object-detection-model-137a349c517b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SDKbX3bD2BZ"
   },
   "outputs": [],
   "source": [
    "config = best_model['Path']\n",
    "plt.rcParams[\"figure.figsize\"] = (30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U5LhISJqWXgM",
    "outputId": "fae4490b-3390-41ae-a909-f40c0df67ded"
   },
   "outputs": [],
   "source": [
    "cfg = load_conf_file(config)\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "for d in random.sample(dataset_dicts, 5):    \n",
    "    print(d[\"file_name\"])\n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    visualizer = Visualizer(im[:, :, ::-1], metadata=swalim_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    #TODO show RGB\n",
    "    print(out.get_image()[:, :, ::-1].shape)\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Swalim_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
