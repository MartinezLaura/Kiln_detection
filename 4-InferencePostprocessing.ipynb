{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Run inference in images\n",
    "\n",
    "In this last step we run all images adquires through the two best models selected by highest F1-score and Recall\n",
    "\n",
    "We transform the information outputed by the net into a georeferenced dataset and apply some operations to handle overlapping objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting paths and bools\n",
    "\n",
    "DIR is where your project path is and RGB is to set if you are working with RGB or with panchromatic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env DIR=/eos/jeodpp/data/projects/REFOCUS/data/swalim_v2\n",
    "\n",
    "%env RGB=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "syspath = \"{}/code/scripts/GDAL-python\".format(os.environ['DIR'])\n",
    "sys.path.append(syspath)\n",
    "import shapefile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# import some common libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from osgeo import osr, ogr, gdal\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the paths where we will save all data\n",
    "\n",
    "This path also contains from the previous step the csv files with the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv('RGB') == 'False':\n",
    "    results_path = '{}/outputs/second_iter/pancro_300/'.format(os.getenv('DIR'))\n",
    "else:\n",
    "    results_path = '{}/outputs/second_iter/rgb_300/'.format(os.getenv('DIR'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some function to run the two best models for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conf_file(path):\n",
    "    config_file_path = path\n",
    "    # The power value applied to image_count when calcualting frequency weight\n",
    "    weights_path = \"{}model_final.pth\".format(path.split('config.yaml')[0])\n",
    "    cfg = get_cfg() \n",
    "    cfg.set_new_allowed(True)\n",
    "\n",
    "    cfg.merge_from_file(config_file_path)\n",
    "    cfg.DATASETS.TRAIN = (\"swalim_train\", )\n",
    "    cfg.DATASETS.TEST = (\"swalim_test\", )\n",
    "    cfg.DATALOADER.NUM_WORKERS = 4\n",
    "    cfg.MODEL.WEIGHTS = weights_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "    return cfg\n",
    "    \n",
    "def Inference(config, path_list_imgs, table, path_copy_im):\n",
    "    cfg = load_conf_file(config)\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    with open(path_list_imgs) as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    for d in lines: \n",
    "        im = cv2.imread(d)\n",
    "        outputs = predictor(im)  \n",
    "        out = outputs[\"instances\"].to(\"cpu\")\n",
    "        box = out.pred_boxes\n",
    "        scores = out.scores\n",
    "        classes = out.pred_classes.tolist() \n",
    "        boxes =  box.tensor.detach().numpy()\n",
    "        print(boxes)\n",
    "        if len(boxes>0):\n",
    "            control = np.zeros((im.shape[0],im.shape[1]), dtype=int)\n",
    "            for i in range(len(boxes)):\n",
    "                control[int(boxes[i][1]): int(boxes[i][3]), int(boxes[i][0]): int(boxes[i][2]),] = 1\n",
    "\n",
    "            name =  '{}{}'.format(path_copy_im, d.split('.tif')[0].split('/')[-1])\n",
    "            shapefile.ArrayToPoly(d,control,name, path_copy_im, config, scores)\n",
    "            \n",
    "def Inference_all(t, df): \n",
    "    #if runned on another platform pick the path and change it,\n",
    "    #if not directly df['Path']\n",
    "    config = df['Path'].replace('/mnt/content/drive/MyDrive/JRC/Swalim_project/swalim_final_clean/outputs/second_iter/pancro_300/', results_path)\n",
    "    \n",
    "    #Inference on non annotated images\n",
    "    if os.getenv('RGB') == 'False':\n",
    "        orig_img_dir = \"{}/inputs/pancro/img_without_ann\".format(os.environ['DIR'])\n",
    "        path_list_imgs = \"{}/inputs/pancro_listwithoutann.csv\".format(os.environ['DIR'])\n",
    "        json_data_path = \"{}/outputs/Inference/pancro_inf/pancro_inf_{}.json\".format(os.environ['DIR'], t)\n",
    "        path_copy_im = \"{}/outputs/Inference/pancro_inf_{}/\".format(os.environ['DIR'], t)\n",
    "        table = \"pancro_{}\".format(t)\n",
    "\n",
    "    else:\n",
    "        orig_img_dir = \"{}/inputs/RGB/img_without_ann\".format(os.environ['DIR'])\n",
    "        path_list_imgs = \"{}/inputs/RGB_listwithoutann.csv\".format(os.environ['DIR'])\n",
    "        json_data_path = \"{}/outputs/Inference/rgb_inf/rgb_inf_{}.json\".format(os.environ['DIR'], t)\n",
    "        path_copy_im = \"{}/outputs/Inference/rgb_inf_{}/\".format(os.environ['DIR'], t)\n",
    "        table = \"rgb_{}\".format(t)\n",
    "        \n",
    "    #creating a new directory called pythondirectory\n",
    "    Path(path_copy_im).mkdir(parents=True, exist_ok=True)\n",
    "    Inference(config, path_list_imgs, table, path_copy_im)    \n",
    "\n",
    "    #Inference on  annotated images\n",
    "    if os.getenv('RGB') == 'False':\n",
    "        orig_img_dir = \"{}/inputs/pancro/img_with_ann\".format(os.environ['DIR'])\n",
    "        path_list_imgs = \"{}/inputs/pancro_listwithann.csv\".format(os.environ['DIR'])\n",
    "        json_data_path = \"{}/outputs/Inference/pancro_inf/pancro_inf_{}.json\".format(os.environ['DIR'], t)\n",
    "        path_copy_im = \"{}/outputs/Inference/pancro_inf_{}/\".format(os.environ['DIR'], t)\n",
    "        table = \"pancro_{}\".format(t)\n",
    "\n",
    "    else:\n",
    "        orig_img_dir = \"{}/inputs/RGB/img_without_ann\".format(os.environ['DIR'])\n",
    "        path_list_imgs = \"{}/inputs/RGB_listwithann.csv\".format(os.environ['DIR'])\n",
    "        json_data_path = \"{}/outputs/Inference/rgb_inf/rgb_inf_{}.json\".format(os.environ['DIR'], t)\n",
    "        path_copy_im = \"{}/outputs/Inference/rgb_inf_{}/\".format(os.environ['DIR'], t)\n",
    "        table = \"rgb_{}\".format(t)\n",
    "        \n",
    "    Path(path_copy_im).mkdir(parents=True, exist_ok=True)\n",
    "    Inference(config, path_list_imgs, table, path_copy_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code to inference in all the AOI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1hSPHutjVkZiCk1h5DSwr7TT7xskxtOLJ"
    },
    "executionInfo": {
     "elapsed": 5893,
     "status": "ok",
     "timestamp": 1633961114883,
     "user": {
      "displayName": "Laura Mart√≠nez",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgGYc4m_Am4sV-KVzZkmTsTJQHhRjoUfd5oOgFVcA=s64",
      "userId": "00638740950867382970"
     },
     "user_tz": -120
    },
    "id": "U5LhISJqWXgM",
    "outputId": "cce6c25c-cd9b-4385-9fd6-ebdbca5fc1ea"
   },
   "outputs": [],
   "source": [
    "#select the bests models from the validation\n",
    "results = pd.read_csv(\"{}/final_results.csv\".format(results_path))\n",
    "results = results.loc[:, ~results.columns.str.contains('^Unnamed')]\n",
    "\n",
    "#select the best two models (f1score and recall)\n",
    "f1_result = results.dropna().sort_values(by=['F_score'], ascending=False).iloc[0]\n",
    "recall_result = results.dropna().sort_values(by=['Recall'], ascending=False).iloc[0]\n",
    "\n",
    "#Inference for f-1score\n",
    "t = 'f1score'\n",
    "Inference_all(t, f1_result)\n",
    "\n",
    "t = 'recall'\n",
    "Inference_all(t, recall_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will do some of the geometrical operations to handle the overlappings between the tiles and datasets.\n",
    "#### Everything will be upload to a postgres database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Postgres info connection:\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "host = \"\"\n",
    "port = \"\"\n",
    "database = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the name of the two tables with the results in postgres:\n",
    "schema = \"swalim\"\n",
    "f1_socre = \"pancro_f1score\"\n",
    "recall = \"pancro_recall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the database:\n",
    "con = psycopg2.connect(user = user,\n",
    "                      password = password,\n",
    "                      host = host,\n",
    "                      port = port,\n",
    "                      database = database)\n",
    "cursor = con.cursor()\n",
    "con.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'CREATE TABLE \"{}\".merged AS( SELECT * FROM \"{}\".{} p UNION SELECT * FROM  \"{}\".{} pb)'.format(\\\n",
    "            schema, schema, f1_score, schema, recall)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "\n",
    "sql = 'DELETE FROM \"{}\".merged c WHERE c.ogc_fid IN ( SELECT b.ogc_fid FROM \"{}\".merged a, \"{}\".merged b'\n",
    "        'WHERE a.ogc_fid < b.ogc_fid AND ST_Intersects(a.wkb_geometry, b.wkb_geometry)'' \n",
    "        'AND ST_Area(ST_Intersection(a.wkb_geometry, b.wkb_geometry)) / ST_Area(a.wkb_geometry) >0.5);'.format(\\\n",
    "            schema, schema)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "if 'pancro' in recall:\n",
    "    name = 'pancro'\n",
    "else:\n",
    "    name = 'rgb'\n",
    "\n",
    "sql = 'ALTER TABLE \"{}\".merged RENAME TO \"{}\".{}_final'.format(schema, schema, name)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "sql = 'ALTER TABLE \"{}\".{}_final ADD COLUMN area double precision'.format(schema, name)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "sql = 'UPDATE \"{}\".{}_final SET area=ST_AREA(\"{}\".{}_final.wkb_geometry)'.format(schema, name, schema, name)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "sql = 'ALTER TABLE \"{}\".{}_final ADD COLUMN diameter double precision'.format(schema, name)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "sql = 'UPDATE \"{}\".{}_final SET diameter=ST_Perimeter(\"{}\".{}_final.wkb_geometry)/4'.format(schema, name, schema, name)\n",
    "\n",
    "cur.execute(sql)\n",
    "\n",
    "con.close ()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Swalim_inference.ipynb",
   "provenance": [
    {
     "file_id": "1AFBNtsamYJgYVr42QFRWl-QZ9gB8to-J",
     "timestamp": 1633342082216
    },
    {
     "file_id": "https://github.com/Tony607/detectron2_instance_segmentation_demo/blob/master/Detectron2_custom_coco_data_segmentation.ipynb",
     "timestamp": 1633082512056
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
